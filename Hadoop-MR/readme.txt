All scripts and required files are put on github. 
Pem file should be transferred manullay to master as it not included in repo due to security reason.


Steps :

Launch d2.xlarge instance of Amazon Linux AMI

Login to the instance 

run command below command to download repo 

--> sudo yum install git 
--> git clone https://github.com/viyatgandhi/hadoopsetup.git

copy pem file to user home directory(home/ec2-user) and hadoopsetup/hadoopforD2 folder
give chmod 400 .pem

run command 

--> eval `ssh-agent -s`
--> ssh-add .pem file

Go to hadoopsetup/hadoopforD2 and run below steps

1. run script prepareraid.sh - create raid0 mount
Ref : http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/raid-config.html

2. run nodeinitonce.sh - update node, install gensort, install hadoop and java and update bashrc file

3. run command to reload config parameters 
--> source ~/.bashrc 

4. edit runclusterscript.sh and copynewslave.sh with your pem file

5. edit masterinput.txt with your master ip

6. run updatexml.sh to update xml with new master ip and with other required parameter for xml

7. Now create AMI of this master and start another 16 nodes from this saved image of master. (No reboot option checked)

8. update slavesinput.txt with all the other slave ip

9. run updateslaves.sh - update slaves in master

10. runclusterscript.sh - transfer slave file to all slaves and do raid0 mount in each slave and start ssh agent 

11. run command to format namenode and it will create dfs of name node on /mnt/raid location 
--> hadoop namenode -format  

12. run start_cluster_v.sh to start the cluster

How to run hadoop TeraSort Program ?

After Cluster is running check the cluster info using 
<masterip>:8088 from browser or run command 

--> hadoop dfsadmin -report

Run below command from path /usr/local/hadoop location 

--> bin/hdfs dfs -mkdir /user
--> bin/hdfs dfs -mkdir /user/ec2-user

Run below command to create input directory in hdfs 

--> hadoop fs -mkdir input 

Put the input.txt file to hdfs where input.txt is generated by gensort from command (gensort -a 100 input.txt)

--> hadoop fs -put input.txt input

Complie java program and run jar using below command to start terasort 

--> hadoop com.sun.tools.javac.Main Tera*.java
--> jar cf TeraSort.jar Tera*.class
--> hadoop jar TeraSort.jar TeraSort input output /user/ec2-user 
Usage:
TeraSort <inputFolder> <outputFolder> <partitionFileFolder>

To check output we need to get data from hdfs 

--> hadoop fs -get output/* .

and do valsort to check sort 

For running long programs run with nohup and & 

Explanation of Code:

Proper inline comments are given below is the overview of the program:

Mapper : simple mapper with Text,Text as input parameter and Text,Text as output.
it divides input line in key - as first 10 bytes
and value as rest of the 90 bytes.
So that hadoop sorts the data using just the key.
Those are set in context object of hadoop.

Reducer: simple reducer just writes the data given by Mapper - it has input as Text,Text 
and output as Text,Text.

Actually if we do not define any mapper or reducer, then also code will work fine 
with just deafult mapper and reducer but we will not have sorting on 10 bytes - rather it will do on all 100 bytes.

Main Application:

-> It will use tool runner to run the Job.
-> You need to give 3 parameters to run the program
-> TeraSort <inputFolder> <outputFolder> <partitionFileFolder>
-> first set the mapper and reducer class to our custom mapper and reducer.
-> You can also set number of reducers in job 
-> As our input files is Text and we want key/value pair as Text,Text we need to use
InputFormatClass as KeyValueTextInputFormat
https://hadoop.apache.org/docs/r2.7.2/api/org/apache/hadoop/mapred/KeyValueTextInputFormat.html

-> Set Map output keys and value to Text as mapper as output Text,Text
-> Set output key class and and value class of reducer to also Text,Text
-> Now most important is to use Total Order Partition which will allow us to have final output as sorted.
-> For that we must define as sampler, here I have used random input sampler of type Text,Text
random sampler has three arguments : freq, numSamples, maxSplitsSampled
freq : Probability with which a key will be chosen
numSamples - Total number of samples to obtain from all selected splits.
maxSplitsSampled - The maximum number of splits to examine
which will take random samples from the input file and which will be used to write the final file.
Take max split number as equal to reducer you are using for good performance.
Becasue for 1 TB we will have more than 7k+ samples if we take samples from each file it will take huge time
https://hadoop.apache.org/docs/r1.0.4/api/org/apache/hadoop/mapreduce/lib/partition/InputSampler.RandomSampler.html
-> Finally set output folder where we need to store the output in hdfs 
 